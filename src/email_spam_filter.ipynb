{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_XI9fvRthZT"
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FcXRMoxxkfH"
   },
   "source": [
    "In this project, I will design and implement several e-mail spam filters using Naive Bayes and SVM based classification on the ling-spam dataset. \n",
    "\n",
    "Here is a list of email spam filters:\n",
    "* Naive Bayes Classifier\n",
    "  * Bernoulli NB classiﬁer with    binary features\n",
    "  * Multinomial NB with binary features\n",
    "  * Multinomial NB with term frequency (TF) features \n",
    "* SVM based Classifier\n",
    "* Adversarial Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FkO9lhtprs"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgGAEo97xrAO"
   },
   "source": [
    "The ling-spam corpus contains e-mails from the Linguist mailing list categorized as either legitimate or spam emails. The corpus is divided into four sub-folders that contain the same emails that are pre-processed with/without lemmatization and with/without stop-word removal. The e-mails in each sub-folder partitioned into 10 folds.\n",
    "\n",
    "In this project, we will use the ﬁrst 9 folds from the ling-spam corpus as **training data**, and the 10th fold as **test data**. Also, we will use the corpus with both lemmatization and stop-word enabled (under the **lemm_stop** folder).\n",
    "\n",
    "Download the ling-spam dataset from: http://www.aueb.gr/users/ion/data/lingspam_public.tar.gz\n",
    "\n",
    "Before we start the experiments, I need to do some preprocessing on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V28eTt1E7wt3"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The purpose of preprocessing is to organize corpus in a clean and unified format and make the following experiments easier. In this step, I will generate 2 csv files **lingspam_train.csv** and **lingspam_test.csv** based on lingspam corpus (lemm_stop folder). The first 9 parts make up the training set and the 10th part is the testing set.\n",
    "\n",
    "In the csv file, it includes all the information we need, such as email subject, email body, is email spam or legitimate.\n",
    "\n",
    "The python script I used to do data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HTZBKnVOCScq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import string\n",
    "\n",
    "\n",
    "TRAINSET_PATH = '../data/train/'\n",
    "TESTSET_PATH = '../data/test/'\n",
    "LINGSPAM_TRAIN_CSV_PATH = TRAINSET_PATH + 'lingspam_train.csv'\n",
    "LINGSPAM_TEST_CSV_PATH = TESTSET_PATH + 'lingspam_test.csv'\n",
    "\n",
    "\n",
    "def generate_trainset(input_dir, output_path):\n",
    "    l = []\n",
    "\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        path = root.split(os.sep)\n",
    "        part_name = os.path.basename(root)\n",
    "        for file in files:\n",
    "            if not file.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            d = {}\n",
    "            file_name = file.replace('.txt', '')\n",
    "            file_path = os.path.join(root, file)\n",
    "            \n",
    "            with codecs.open(file_path, mode='r', encoding='utf8', errors='ignore') as f:\n",
    "                line_counter = 0\n",
    "                for line in f.readlines():\n",
    "                    line = line.strip()\n",
    "                    if line_counter == 0:   # subject\n",
    "                        subject = line.replace('Subject:', '').strip()\n",
    "                    if line_counter == 2:\n",
    "                        email = line\n",
    "                    line_counter += 1\n",
    "            d['email_subject'] = subject\n",
    "            d['email_body'] = email\n",
    "            d['part_name'] = part_name\n",
    "            d['file_name'] = file_name\n",
    "            d['is_spam'] = 1 if file_name.startswith('spmsg') else 0\n",
    "            l.append(d)\n",
    "    \n",
    "    with codecs.open(output_path, mode='w', encoding='utf8', errors='ignore') as out_file:\n",
    "        writer = csv.DictWriter(out_file, l[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in l:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# generate_trainset(TRAINSET_PATH, LINGSPAM_TRAIN_CSV_PATH)\n",
    "# generate_trainset(TESTSET_PATH, LINGSPAM_TEST_CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5ttlnQxCePW"
   },
   "source": [
    "We can't really run the above code in this notebook since it's hard to upload the dataset folders to colab. So I run this script offline and generated 2 expected csv files. I put the link here\n",
    "* [lingspam_train.csv](https://drive.google.com/file/d/14dLhYauFUwm7a8bonGr_8MoQRCy_ufpA/view?usp=sharing)\n",
    "* [lingspan_test.csv](https://drive.google.com/file/d/1R4PgItvVQ-pZ3IES2YDSFI5wGxLUZmz_/view?usp=sharing)\n",
    "\n",
    "Please download these 2 files and upload them to this colab notebook, click the upload button on the left Files tab.\n",
    "\n",
    "Now we have cleaned corpus, let's take a look at them using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\DELL\\OneDrive\\Desktop\\Project NOV01\\Email scamcheck\\artificial-intelligence\\email-spam-filter\\src\n",
      "Train Data:\n",
      "                                  email_subject  \\\n",
      "0          job post - apple-iss research center   \n",
      "1                                           NaN   \n",
      "2  query : letter frequency text identification   \n",
      "3                                          risk   \n",
      "4                      request book information   \n",
      "\n",
      "                                          email_body part_name   file_name  \\\n",
      "0  content - length : 3386 apple-iss research cen...     part3   6-110msg3   \n",
      "1  lang classification grime , joseph e . barbara...     part3   6-126msg1   \n",
      "2  post inquiry sergeus atama ( satama @ umabnet ...     part3  6-1125msg2   \n",
      "3  colleague research differ degree risk perceive...     part3  6-1157msg2   \n",
      "4  earlier morn phone friend mine live south amer...     part3  6-1147msg2   \n",
      "\n",
      "   is_spam  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "Test Data:\n",
      "                                       email_subject  \\\n",
      "0                          semitic language workshop   \n",
      "1  endanger language - edinburgh , sept 98 - call...   \n",
      "2                 workshop sposs preliminary program   \n",
      "3                             nels 29 - - call paper   \n",
      "4                                 kornfilt : turkish   \n",
      "\n",
      "                                          email_body part_name  file_name  \\\n",
      "0  workshop computational approach semitic langua...    part10  9-920msg1   \n",
      "1  endanger language - role specialist ? - - - - ...    part10  9-858msg1   \n",
      "2  sposs sound pattern spontaneous speech product...    part10  9-989msg1   \n",
      "3  * * * * * * * * * * * * n e l s 29 * * * * * *...    part10  9-697msg2   \n",
      "4  jaklin kornfilt ( 1997 ) , turkish . london yo...    part10  9-645msg1   \n",
      "\n",
      "   is_spam  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Construct the relative path to the data folder\n",
    "data_folder_path = os.path.join('..', 'data')\n",
    "\n",
    "# Construct full paths to your train and test data files\n",
    "train_data_path = os.path.join(data_folder_path, 'train/lingspam_train.csv')\n",
    "test_data_path = os.path.join(data_folder_path, 'test/lingspam_test.csv')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    lingspam_train_df = pd.read_csv(train_data_path)\n",
    "    lingspam_test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Display the first few rows of the data\n",
    "    print(\"Train Data:\")\n",
    "    print(lingspam_train_df.head())\n",
    "\n",
    "    print(\"Test Data:\")\n",
    "    print(lingspam_test_df.head())\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "06NadHmFDkmU",
    "outputId": "75a94ba2-0ecd-4d23-8b09-0753f32a078f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset column names:\n",
      "email_subject\n",
      "email_body\n",
      "part_name\n",
      "file_name\n",
      "is_spam\n",
      "\n",
      "lingspam trainset:\n",
      "                                          email_subject  \\\n",
      "0                  job post - apple-iss research center   \n",
      "1                                                   NaN   \n",
      "2          query : letter frequency text identification   \n",
      "3                                                  risk   \n",
      "4                              request book information   \n",
      "...                                                 ...   \n",
      "2597                            love profile - ysuolvpv   \n",
      "2598                                    ask join kiddin   \n",
      "2599                      anglicization composer ' name   \n",
      "2600  re : 6 . 797 , comparative method : n - ary co...   \n",
      "2601                 re : american - english australium   \n",
      "\n",
      "                                             email_body part_name   file_name  \\\n",
      "0     content - length : 3386 apple-iss research cen...     part3   6-110msg3   \n",
      "1     lang classification grime , joseph e . barbara...     part3   6-126msg1   \n",
      "2     post inquiry sergeus atama ( satama @ umabnet ...     part3  6-1125msg2   \n",
      "3     colleague research differ degree risk perceive...     part3  6-1157msg2   \n",
      "4     earlier morn phone friend mine live south amer...     part3  6-1147msg2   \n",
      "...                                                 ...       ...         ...   \n",
      "2597  hello thank stop ! ! many pic hot video ! most...     part8   spmsgc134   \n",
      "2598  list owner : \" kiddin \" invite join mail list ...     part8   spmsgc108   \n",
      "2599  judge return post , must sound kind self-procl...     part8  8-1119msg1   \n",
      "2600  gotcha ! two separate fallacy argument against...     part8   6-825msg3   \n",
      "2601  hello ! ' m work thesis concern attitude towar...     part8   6-813msg1   \n",
      "\n",
      "      is_spam  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "...       ...  \n",
      "2597        1  \n",
      "2598        1  \n",
      "2599        0  \n",
      "2600        0  \n",
      "2601        0  \n",
      "\n",
      "[2602 rows x 5 columns]\n",
      "\n",
      "lingspam testset:\n",
      "                                         email_subject  \\\n",
      "0                            semitic language workshop   \n",
      "1    endanger language - edinburgh , sept 98 - call...   \n",
      "2                   workshop sposs preliminary program   \n",
      "3                               nels 29 - - call paper   \n",
      "4                                   kornfilt : turkish   \n",
      "..                                                 ...   \n",
      "286                           sla conference pari 1999   \n",
      "287                            conference announcement   \n",
      "288                                           semantic   \n",
      "289                          honor two keynote speaker   \n",
      "290                                           typology   \n",
      "\n",
      "                                            email_body part_name  file_name  \\\n",
      "0    workshop computational approach semitic langua...    part10  9-920msg1   \n",
      "1    endanger language - role specialist ? - - - - ...    part10  9-858msg1   \n",
      "2    sposs sound pattern spontaneous speech product...    part10  9-989msg1   \n",
      "3    * * * * * * * * * * * * n e l s 29 * * * * * *...    part10  9-697msg2   \n",
      "4    jaklin kornfilt ( 1997 ) , turkish . london yo...    part10  9-645msg1   \n",
      "..                                                 ...       ...        ...   \n",
      "286  call papers xi th international conference \" a...    part10  9-640msg1   \n",
      "287  southern illinoi university edwardsville carbo...    part10  9-738msg1   \n",
      "288  bring attention two publication john benjamin ...    part10  9-886msg1   \n",
      "289  international conference natural language proc...    part10  9-947msg1   \n",
      "290  bring attention recent publication john benjam...    part10  9-957msg1   \n",
      "\n",
      "     is_spam  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "..       ...  \n",
      "286        0  \n",
      "287        0  \n",
      "288        0  \n",
      "289        0  \n",
      "290        0  \n",
      "\n",
      "[291 rows x 5 columns]\n",
      "\n",
      "Trainset size: 2602\n",
      "Testset size: 291\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "dtype={\n",
    "    'email_subject': str,\n",
    "    'email_body': str,\n",
    "    'part_name': str,\n",
    "    'file_name':str,\n",
    "    'is_spam': int\n",
    "    }\n",
    "\n",
    "print(\"Dataset column names:\")\n",
    "for col in lingspam_train_df.columns:\n",
    "  print(col)\n",
    "\n",
    "print('\\nlingspam trainset:')\n",
    "print(lingspam_train_df)\n",
    "print('\\nlingspam testset:')\n",
    "print(lingspam_test_df)\n",
    "\n",
    "trainset_size = lingspam_train_df.shape[0]\n",
    "testset_size = lingspam_test_df.shape[0]\n",
    "print(\"\\nTrainset size: \" + str(trainset_size))\n",
    "print(\"Testset size: \" + str(testset_size))\n",
    "\n",
    "y_train = lingspam_train_df['is_spam'].to_numpy()\n",
    "y_test = lingspam_test_df['is_spam'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvX8qtE-H4T0"
   },
   "source": [
    "From the shape of csv dataframes, the trainset has 2602 samples, the testset has 291 test cases.\n",
    "\n",
    "Next, I will make an dictionary from the trainset **lingspam_train.csv** to count word frequency. With this dictonary, we can easily extract binary features or term frequency features from the trainset and testset.\n",
    "\n",
    "For the eamil body, I remove all the punctuations since I believe they do not affect whether the mail is spam. Also, words which length less than 2 are removed since I don't think single letters are real words. Besides, I only keep those words which all the characters are alphabet letters (a-z).\n",
    "\n",
    "For the feature selection step, we don't want the dimension of our features is too large since it will bring too much computational cost. There are about 55k unique words in lingspam training dataset, we will do some feature selection based on information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "JoZ4Z79o28Eo",
    "outputId": "86d8ef9f-f75d-478e-de75-99d16998abc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words in lingspam trainset: 44664\n",
      "The total times they appeared: 660163\n",
      "The 20 most common words in trainset:\n",
      "('language', 7259)\n",
      "('university', 5271)\n",
      "('linguistic', 2890)\n",
      "('de', 2849)\n",
      "('address', 2778)\n",
      "('one', 2702)\n",
      "('information', 2646)\n",
      "('send', 2232)\n",
      "('order', 2206)\n",
      "('conference', 2131)\n",
      "('work', 2056)\n",
      "('english', 2052)\n",
      "('please', 2018)\n",
      "('include', 1995)\n",
      "('mail', 1985)\n",
      "('email', 1956)\n",
      "('program', 1895)\n",
      "('name', 1793)\n",
      "('http', 1786)\n",
      "('paper', 1770)\n",
      "\n",
      "The length of current dictionary: 44664\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "dictionary = {}\n",
    "\n",
    "for index, row in lingspam_train_df.iterrows():\n",
    "  email = row['email_body'].split(' ')\n",
    "  email = [word for word in email if word not in string.punctuation]\n",
    "  email = [word for word in email if len(word) > 1]\n",
    "  email = [word for word in email if word.isalpha() == True]\n",
    "  words += email\n",
    "\n",
    "dictionary = Counter(words)\n",
    "\n",
    "unique_num = len(dictionary)\n",
    "total_num = sum(dictionary.values())\n",
    "\n",
    "print(\"The number of unique words in lingspam trainset: \" + str(unique_num))\n",
    "print(\"The total times they appeared: \" + str(total_num))\n",
    "\n",
    "print(\"The 20 most common words in trainset:\")\n",
    "print(*dictionary.most_common(20), sep='\\n')\n",
    "\n",
    "print('\\nThe length of current dictionary: ' + str(len(dictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgTx4XpA81ui"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlD_mFgw8B6g"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw7Abkvu_8I6"
   },
   "source": [
    "In the next 3 Naive Bayes classifiers, we will use 2 ways to encode features:\n",
    "* Binary/Boolean Features\n",
    "  * $x_i=1$ if term $i$ appears in a doc; $0$ otherwise\n",
    "  * Each doc has a $M$ dimension boolean features, $x = (x_1, x_2, ..., x_M)$ \n",
    "* Term Frequencies (TF) Features\n",
    "  * $x_i$: number of times term $i$ appears in a doc\n",
    "  * Each doc is represented by $x = (x_1, x_2, ... x_M)$, a vector of term frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL2IjHn83eSK"
   },
   "source": [
    "**Information gain** (IG) will be used as metric to perform feature selection (rank features). Here, IG metric only accounts for the occurrence of (and not frequency with which terms appear) in the dataset. Also, I use **Laplacian Smoothing** to ensure that there is no event with zero probability.\n",
    "\n",
    "Next let's calculate [IG](https://drive.google.com/file/d/1jLnCu_e7dmMIhLXgTLkRemo3U6rvZ_kc/view?usp=sharing) for words in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Wi-sVzmS4KIc",
    "outputId": "353f10a3-e1d5-4641-b233-e2f0df0b7189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total legit email number = 2170.0\n",
      "Total spam email number = 432.0\n",
      "p = 0.8339738662567256\n",
      "H(C) = 0.6485330171848535\n",
      "word: language, info_gain: 0.19967490044495856\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total_legit_emails = 0.0\n",
    "total_spam_emails = 0.0\n",
    "\n",
    "for index, row in lingspam_train_df.iterrows():\n",
    "  is_spam = row['is_spam']\n",
    "  if is_spam == 1:\n",
    "    total_spam_emails += 1\n",
    "  else:\n",
    "    total_legit_emails += 1\n",
    "\n",
    "print(\"Total legit email number = {}\".format(total_legit_emails))\n",
    "print(\"Total spam email number = {}\".format(total_spam_emails))\n",
    "\n",
    "p = total_legit_emails / (total_spam_emails + total_legit_emails)\n",
    "print(\"p = {}\".format(p))\n",
    "\n",
    "h_c = -1 * p * math.log(p, 2) - (1 - p) * math.log(1 - p, 2)\n",
    "print(\"H(C) = {}\".format(h_c))\n",
    "\n",
    "\n",
    "def count_legit_emails_with_word(word):\n",
    "  num_legit_emails_with_word = 0\n",
    "  for index, row in lingspam_train_df.iterrows():\n",
    "    if row['is_spam'] == 0 and word in row['email_body'].split(' '):\n",
    "      num_legit_emails_with_word += 1\n",
    "  return num_legit_emails_with_word\n",
    "      \n",
    "def count_spam_emails_with_word(word):\n",
    "  num_spam_emails_with_word = 0\n",
    "  for index, row in lingspam_train_df.iterrows():\n",
    "    if row['is_spam'] == 1 and word in row['email_body'].split(' '):\n",
    "      num_spam_emails_with_word += 1\n",
    "  return num_spam_emails_with_word\n",
    "\n",
    "def h_legit_word_not_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return (total_legit_emails - num_legit_emails_with_word) / (total_spam_emails + total_legit_emails) * math.log((total_legit_emails - num_legit_emails_with_word) / (total_spam_emails - num_spam_emails_with_word + total_legit_emails - num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_spam_word_not_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return (total_spam_emails - num_spam_emails_with_word) / (total_spam_emails + total_legit_emails) * math.log((total_spam_emails - num_spam_emails_with_word) / (total_spam_emails - num_spam_emails_with_word + total_legit_emails - num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_legit_word_is_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return num_legit_emails_with_word / (total_spam_emails + total_legit_emails) * math.log(num_legit_emails_with_word / (num_spam_emails_with_word + num_legit_emails_with_word), 2)\n",
    "\n",
    "def h_spam_word_is_present(word):\n",
    "  num_legit_emails_with_word = count_legit_emails_with_word(word)\n",
    "  num_spam_emails_with_word = count_spam_emails_with_word(word)\n",
    "  return num_spam_emails_with_word / (total_spam_emails + total_legit_emails) * math.log(num_spam_emails_with_word / (num_spam_emails_with_word + num_legit_emails_with_word), 2)\n",
    "\n",
    "def info_gain(word):\n",
    "  h_c_x = -1 * (h_legit_word_not_present(word) + h_spam_word_not_present(word) + h_legit_word_is_present(word) + h_spam_word_is_present(word))\n",
    "  ig = h_c - h_c_x\n",
    "  return ig\n",
    "\n",
    "word = \"language\"\n",
    "print(\"word: {}, info_gain: {}\".format(word, info_gain(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quDxLxZ-sBMV"
   },
   "source": [
    "Since the computation time for all words in dataset (45k words!) is quite long, I did the computation offline and save the dictionary with information gain values to a csv file. You can download this file from this [link](https://drive.google.com/file/d/1aCKYVrZ-q4shiXyAfSxqZ06cMLEzwDn0/view?usp=sharing). After you download it, upload it to this colab notebook and we will need this in the following experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "dQDswlW8swmC",
    "outputId": "98414507-3d9e-4fd9-93e8-3c95e238b697"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Information Gain dictionary:\n",
      "             word  freq        ig\n",
      "0        language  7259  0.199639\n",
      "1      university  5271  0.144992\n",
      "2      linguistic  2890  0.145682\n",
      "3              de  2849  0.049473\n",
      "4         address  2778  0.007449\n",
      "...           ...   ...       ...\n",
      "44659    confusio     1 -0.000124\n",
      "44660      mathce     1 -0.000124\n",
      "44661  pharmacist     1 -0.000124\n",
      "44662       lolly     1 -0.000124\n",
      "44663     jessica     1 -0.000124\n",
      "\n",
      "[44664 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ig_filepath= os.path.join(data_folder_path, 'ig.csv')\n",
    "\n",
    "dtype={\n",
    "    'word': str,\n",
    "    'freq': int,\n",
    "    'ig': float,\n",
    "    }\n",
    "\n",
    "ig_df = pd.read_csv(ig_filepath, dtype=dtype)\n",
    "\n",
    "\n",
    "print('\\nInformation Gain dictionary:')\n",
    "print(ig_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LtVupAqRGjE"
   },
   "source": [
    "Now we get Information Gain values for each word in the dataset, we are able to perform feature selection by ranking word on descending order based on IG. From the training data, select the top-N features ($N = {10, 100, 1000}$) on terms of the highest Information Gain (IG) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "klPSo5v6x0Au",
    "outputId": "879c4053-0970-486f-c657-06a4a0d1a8d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary sorted by IG on desending order:\n",
      "                word  freq        ig\n",
      "0           language  7259  0.199639\n",
      "239           remove   437  0.168905\n",
      "48              free  1017  0.159972\n",
      "2         linguistic  2890  0.145682\n",
      "1         university  5271  0.144992\n",
      "...              ...   ...       ...\n",
      "29422        loveday     1 -0.000124\n",
      "29421  transferencia     1 -0.000124\n",
      "29420         douaud     1 -0.000124\n",
      "29419          accra     1 -0.000124\n",
      "44663        jessica     1 -0.000124\n",
      "\n",
      "[44664 rows x 3 columns]\n",
      "\n",
      "Top-10 features:\n",
      "           word  freq        ig\n",
      "0      language  7259  0.199639\n",
      "239      remove   437  0.168905\n",
      "48         free  1017  0.159972\n",
      "2    linguistic  2890  0.145682\n",
      "1    university  5271  0.144992\n",
      "64        money   916  0.118676\n",
      "529       click   251  0.101160\n",
      "186      market   509  0.091497\n",
      "22          our  1707  0.087068\n",
      "87     business   809  0.083016\n",
      "\n",
      "Top-100 features:\n",
      "            word  freq        ig\n",
      "0       language  7259  0.199639\n",
      "239       remove   437  0.168905\n",
      "48          free  1017  0.159972\n",
      "2     linguistic  2890  0.145682\n",
      "1     university  5271  0.144992\n",
      "...          ...   ...       ...\n",
      "40         study  1232  0.035863\n",
      "27      workshop  1613  0.035568\n",
      "1318    security    91  0.035187\n",
      "66      analysis   905  0.034641\n",
      "467          off   277  0.034402\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "Top-1000 features:\n",
      "            word  freq        ig\n",
      "0       language  7259  0.199639\n",
      "239       remove   437  0.168905\n",
      "48          free  1017  0.159972\n",
      "2     linguistic  2890  0.145682\n",
      "1     university  5271  0.144992\n",
      "...          ...   ...       ...\n",
      "532       effect   249  0.006954\n",
      "599        north   221  0.006920\n",
      "1588   beautiful    72  0.006910\n",
      "1494        self    77  0.006910\n",
      "757          dan   172  0.006901\n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "\n",
      "Top-10 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business']\n",
      "\n",
      "Top-100 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business', 'today', 'product', 'advertise', 'company', 'sell', 'english', 'linguistics', 'million', 'income', 'internet', 'day', 'guarantee', 'thousand', 'save', 'easy', 'over', 'best', 'purchase', 'win', 'check', 'buy', 'bulk', 'want', 'cash', 'dollar', 'cost', 'every', 'edu', 'll', 'mailing', 'service', 'com', 'yourself', 'papers', 'hour', 'linguist', 'hundred', 'theory', 'earn', 'profit', 'customer', 'de', 'abstract', 'success', 'fun', 'offer', 'month', 'yours', 'conference', 'receive', 'ever', 'watch', 'speaker', 'bonus', 'mail', 'discussion', 'credit', 'zip', 'here', 'pay', 'live', 'amaze', 'sale', 'syntax', 'department', 'start', 'investment', 'online', 'anywhere', 'toll', 'grammar', 'dream', 'huge', 'financial', 'science', 'ad', 'deadline', 'simply', 'week', 'structure', 'friend', 'mlm', 'need', 'wait', 'fresh', 'study', 'workshop', 'security', 'analysis', 'off']\n",
      "\n",
      "Top-1000 words:\n",
      "['language', 'remove', 'free', 'linguistic', 'university', 'money', 'click', 'market', 'our', 'business', 'today', 'product', 'advertise', 'company', 'sell', 'english', 'linguistics', 'million', 'income', 'internet', 'day', 'guarantee', 'thousand', 'save', 'easy', 'over', 'best', 'purchase', 'win', 'check', 'buy', 'bulk', 'want', 'cash', 'dollar', 'cost', 'every', 'edu', 'll', 'mailing', 'service', 'com', 'yourself', 'papers', 'hour', 'linguist', 'hundred', 'theory', 'earn', 'profit', 'customer', 'de', 'abstract', 'success', 'fun', 'offer', 'month', 'yours', 'conference', 'receive', 'ever', 'watch', 'speaker', 'bonus', 'mail', 'discussion', 'credit', 'zip', 'here', 'pay', 'live', 'amaze', 'sale', 'syntax', 'department', 'start', 'investment', 'online', 'anywhere', 'toll', 'grammar', 'dream', 'huge', 'financial', 'science', 'ad', 'deadline', 'simply', 'week', 'structure', 'friend', 'mlm', 'need', 'wait', 'fresh', 'study', 'workshop', 'security', 'analysis', 'off', 'marketing', 'ship', 'topic', 'cd', 'instruction', 'fantastic', 'profitable', 'package', 'back', 're', 'legal', 'excite', 'spend', 'net', 'research', 'step', 'site', 'freedom', 'issue', 'committee', 'debt', 'price', 'discourse', 'immediately', 'xxx', 'development', 'delete', 'advertisement', 'keep', 'everything', 'aol', 'semantic', 'overnight', 'reference', 'line', 'delivery', 'again', 'right', 'reply', 'orders', 'sales', 'spam', 'home', 'never', 'secret', 'theoretical', 'simple', 'fill', 'hit', 'resell', 'speech', 'enter', 'ac', 'author', 'once', 'stealth', 'tax', 'next', 'adult', 'tel', 'risk', 'down', 'acquisition', 'unlimit', 'list', 'amount', 'campaign', 'phonology', 'message', 'card', 'computational', 'bill', 'own', 'aspect', 'partner', 'cognitive', 'don', 'monthly', 'bank', 'amazing', 'plans', 'hottest', 'undeliverable', 'video', 'lexical', 'submission', 'visa', 've', 'absolutely', 'love', 'super', 'return', 'semantics', 'personal', 'fax', 'works', 'invest', 'student', 'opportunity', 'lottery', 'hello', 'big', 'context', 'capital', 'choose', 'hot', 'weekly', 'french', 'engine', 'great', 'per', 'car', 'life', 'is', 'add', 'relax', 'join', 'prove', 'affiliation', 'reports', 'remember', 'between', 'top', 'effective', 'institute', 'guaranteed', 'hundreds', 'hobby', 'focus', 'nothing', 'us', 'syntactic', 'tell', 'exactly', 'construction', 'luck', 'brand', 'verb', 'try', 'cent', 'refund', 'retire', 'generate', 'extra', 'total', 'comply', 'quick', 'envelope', 'put', 'translation', 'corporations', 'everythe', 'millions', 'dollars', 'worldwide', 'morphology', 'decide', 'lose', 'fortune', 'rate', 'federal', 'faster', 'totally', 'future', 'april', 'lists', 'duplicate', 'everyone', 'practically', 'german', 'role', 'seven', 'paper', 'sources', 'pp', 'greatest', 'gamble', 'invite', 'even', 'create', 'window', 'chance', 'powerful', 'owe', 'perspective', 'sender', 'always', 'john', 'researcher', 'competition', 'mastercard', 'advantage', 'sure', 'bankruptcy', 'paste', 'completely', 'suite', 'much', 'chair', 'european', 'session', 'word', 'pragmatic', 'legitimate', 'teen', 'expiration', 'evidence', 'speak', 'sex', 'august', 'grammatical', 'historical', 'charge', 'mailbox', 'excess', 'city', 'billion', 'lucky', 'modern', 'order', 'summary', 'hesitate', 'programme', 'true', 'search', 'alter', 'enjoy', 'evaluating', 'yahoo', 'miss', 'code', 'tips', 'downline', 'millionaire', 'sentence', 'eliminate', 'trade', 'programs', 'literature', 'clean', 'recruit', 'visit', 'wrap', 'afford', 'sincerely', 'formal', 'are', 'vacation', 'junk', 'truly', 'aim', 'corpus', 'plus', 'secure', 'mci', 'scam', 'mortgage', 'intrusion', 'discuss', 'unsubscribe', 'nl', 'honest', 'deposit', 'quit', 'interaction', 'easiest', 'print', 'most', 'representation', 'away', 'better', 'presentation', 'really', 'datum', 'submit', 'ye', 'signature', 'native', 'relevant', 'academic', 'addresses', 'constraint', 'stop', 'dialect', 'incredible', 'testimonial', 'cleanest', 'overload', 'earnings', 'capitalfm', 'fabulous', 'imagine', 'roll', 'wish', 'description', 'law', 'verify', 'instructions', 'forever', 'society', 'effort', 'shop', 'pick', 'organize', 'registration', 'else', 'id', 'succeed', 'general', 'theme', 'approach', 'job', 'query', 'variety', 'modem', 'hours', 'services', 'worth', 'culture', 'gold', 'software', 'relation', 'phonological', 'filter', 'girl', 'bottom', 'germany', 'ticket', 'astonishment', 'released', 'reg', 'sexually', 'staggering', 'numbers', 'instant', 'totals', 'cram', 'creditor', 'yes', 'game', 'protection', 'mailer', 'sociolinguistic', 'publication', 'trial', 'lexicon', 'family', 'sit', 'phone', 'piece', 'protect', 'pattern', 'argument', 'notification', 'ed', 'beach', 'accurately', 'believer', 'raleigh', 'started', 'vanish', 'release', 'easily', 'interpretation', 'proof', 'let', 'fast', 'instruct', 'speed', 'many', 'movie', 'batch', 'upgrade', 'conceal', 'accountant', 'ordering', 'van', 'financially', 'laugh', 'ez', 'fairchild', 'anytime', 'juno', 'spokane', 'webmaster', 'spout', 'entrepreneur', 'desirous', 'merciless', 'grumble', 'letter', 'book', 'industry', 'forget', 'contribution', 'believe', 'framework', 'cambridge', 'discover', 'present', 'tip', 'doubt', 'please', 'run', 'automatically', 'real', 'proceedings', 'france', 'entire', 'introduction', 'marketer', 'lawful', 'hardcore', 'earth', 'particular', 'name', 'spanish', 'latest', 'allow', 'corporation', 'faith', 'isp', 'fraction', 'limited', 'unsolicit', 'extraordinary', 'someone', 'waste', 'help', 'ours', 'retirement', 'buyer', 'length', 'mit', 'complex', 'et', 'rockland', 'stun', 'making', 'weeks', 'unlimited', 'estate', 'esq', 'pass', 'enterprise', 'noun', 'perfectly', 'rights', 'before', 'alone', 'comparative', 'natural', 'expression', 'isbn', 'welcome', 'advertiser', 'compliance', 'extractor', 'genie', 'variation', 'uk', 'among', 'server', 'did', 'reap', 'days', 'clearance', 'little', 'revenue', 'wall', 'sent', 'amateur', 'penny', 'rom', 'below', 'association', 'owner', 'additional', 'turn', 'read', 'prodigy', 'refinance', 'prepared', 'dupe', 'retail', 'thousands', 'privacy', 'robbery', 'awesome', 'removed', 'phonetic', 'office', 'gov', 'professor', 'download', 'faculty', 'cultural', 'proposal', 'professional', 'alway', 'air', 'client', 'color', 'editor', 'particularly', 'acceptance', 'payable', 'chapter', 'chat', 'march', 'boy', 'magazine', 'dictionary', 'principle', 'qualify', 'japanese', 'mclaughlin', 'savings', 'less', 'percentage', 'whatsoever', 'report', 'chinese', 'type', 'relate', 'increase', 'leave', 'argue', 'au', 'toy', 'rat', 'removal', 'ram', 'trash', 'blvd', 'centre', 'robert', 'dissertation', 'inexpensive', 'potential', 'high', 'article', 'action', 'quickly', 'news', 'exclusive', 'notion', 'morphological', 'reach', 'anything', 'shock', 'classified', 'boyfriend', 'emailer', 'living', 'secrets', 'filled', 'celebrity', 'msn', 'colleague', 'parallel', 'store', 'daily', 'am', 'button', 'subject', 'show', 'february', 'unique', 'discipline', 'distinction', 'lecture', 'cut', 'postage', 'low', 'each', 'blast', 'instantly', 'meg', 'residual', 'benefits', 'casino', 'david', 'skeptical', 'lifetime', 'persistent', 'hold', 'rush', 'bet', 'gift', 'application', 'clause', 'richer', 'goods', 'cds', 'access', 'state', 'logic', 'tremendous', 'convenience', 'provider', 'already', 'lot', 'thing', 'request', 'scholar', 'vowel', 'why', 'der', 'symposium', 'porn', 'wilburn', 'seller', 'inflation', 'someday', 'vulgarity', 'premium', 'pile', 'stamped', 'included', 'proven', 'soon', 'after', 'winner', 'proud', 'file', 'netherland', 'dept', 'sexual', 'golden', 'second', 'until', 'style', 'graduate', 'almost', 'organizer', 'psycholinguistic', 'project', 'recent', 'hand', 'early', 'empirical', 'berlin', 'ready', 'michael', 'hotmail', 'lanse', 'jackson', 'campus', 'immediate', 'participate', 'happen', 'illegal', 'amex', 'cloth', 'ling', 'fastest', 'confidential', 'dial', 'countless', 'embark', 'cable', 'through', 'june', 'ahead', 'jump', 'trust', 'postscript', 'dutch', 'teacher', 'case', 'info', 'move', 'small', 'nc', 'enclose', 'term', 'accommodation', 'consist', 'text', 'test', 'deliver', 'ease', 'exact', 'rather', 'methodology', 'generative', 'jame', 'few', 'operate', 'object', 'catchy', 'downpayment', 'cum', 'recession', 'wisely', 'profanity', 'mega', 'flamer', 'instructed', 'poorer', 'divorce', 'billboard', 'pic', 'biz', 'murkowskus', 'overflow', 'unproductive', 'japan', 'oxford', 'central', 'implication', 'hr', 'model', 'ph', 'phrase', 'publish', 'require', 'define', 'promotion', 'largest', 'stock', 'expensive', 'loan', 'dr', 'obligation', 'thereafter', 'newsgroup', 'bel', 'entertainment', 'martin', 'psychology', 'laboratory', 'hall', 'exp', 'trail', 'postmaster', 'girlfriend', 'affordable', 'unemployment', 'journal', 'september', 'cyber', 'sleep', 'legally', 'recieve', 'gay', 'recipient', 'goodness', 'promptly', 'upset', 'driver', 'awhile', 'continual', 'typology', 'phonetics', 'fr', 'pennsylvanium', 'goal', 'peter', 'carefully', 'obviously', 'scientific', 'average', 'error', 'proceeding', 'derive', 'mark', 'extend', 'verbal', 'asset', 'extremely', 'dialogue', 'preliminary', 'organise', 'biggest', 'human', 'everyday', 'promise', 'november', 'plenary', 'keynote', 'within', 'using', 'drop', 'hypothesis', 'kid', 'various', 'radio', 'husband', 'flame', 'fl', 'pop', 'successful', 'tense', 'originator', 'requesting', 'assuming', 'moving', 'authenticate', 'mba', 'platinum', 'checks', 'commercialemail', 'respectability', 'cards', 'vcs', 'unforeseen', 'selle', 'profits', 'sexiest', 'happy', 'address', 'payment', 'compuserve', 'pronoun', 'accompany', 'edinburgh', 'parse', 'utility', 'record', 'school', 'prof', 'paradise', 'competitor', 'mouse', 'encourage', 'assumption', 'panel', 'spain', 'bibliography', 'convince', 'bit', 'install', 'wealth', 'conservative', 'satisfy', 'star', 'susan', 'scope', 'generally', 'race', 'volume', 'represent', 'correctly', 'manufacturer', 'dare', 'comfort', 'consumer', 'log', 'identify', 'contrast', 'gender', 'reception', 'concern', 'excellent', 'gain', 'erotic', 'exceedingly', 'adults', 'delphus', 'friends', 'favourite', 'anon', 'banner', 'paid', 'apply', 'philosophy', 'en', 'email', 'muncie', 'fortunately', 'criminal', 'permanently', 'album', 'kitchen', 'anonymous', 'insurance', 'mouton', 'univ', 'hard', 'suggest', 'movement', 'function', 'moment', 'box', 'launch', 'highway', 'dave', 'snail', 'md', 'hi', 'patient', 'functional', 'prompt', 'effect', 'north', 'beautiful', 'self', 'dan']\n"
     ]
    }
   ],
   "source": [
    "sorted_ig_df = ig_df.sort_values(by=['ig'], ascending=False)\n",
    "\n",
    "print('\\nDictionary sorted by IG on desending order:')\n",
    "print(sorted_ig_df)\n",
    "\n",
    "top_10_features = sorted_ig_df.head(10)\n",
    "top_100_features = sorted_ig_df.head(100)\n",
    "top_1000_features = sorted_ig_df.head(1000)\n",
    "\n",
    "print(\"\\nTop-10 features:\")\n",
    "print(top_10_features)\n",
    "print(\"\\nTop-100 features:\")\n",
    "print(top_100_features)\n",
    "print(\"\\nTop-1000 features:\")\n",
    "print(top_1000_features)\n",
    "\n",
    "top_10_features_list = [row['word'] for index, row in top_10_features.iterrows()]\n",
    "top_100_features_list = [row['word'] for index, row in top_100_features.iterrows()]\n",
    "top_1000_features_list = [row['word'] for index, row in top_1000_features.iterrows()]\n",
    "\n",
    "print(\"\\nTop-10 words:\")\n",
    "print(top_10_features_list)\n",
    "print(\"\\nTop-100 words:\")\n",
    "print(top_100_features_list)\n",
    "print(\"\\nTop-1000 words:\")\n",
    "print(top_1000_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21SqfAQJ0m_F"
   },
   "source": [
    "Now we have 3 feature dictonaries, top-10, top-100 and top-1000. Next we can extract binary feature matrixes or term frequency feature matrixes from datasets based on them. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkf3kP9o8NWy"
   },
   "source": [
    "## Bernoulli NB classiﬁer with Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnujaHdg_8zP"
   },
   "source": [
    "[Bernoulli Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) for multivariate Bernoulli models. This classifier is suitable for discrete data. BernoulliNB is designed for binary/boolean features. Here, bianry feature $x_i = 1$ if word $i$ appears in a document/email, 0 otherwise.\n",
    "\n",
    "Next I will extract binary feature matrix from trainset/testset based on the top-N feature dictonaries calculated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pPBg-O6CzVZu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "def extract_binary_features(df: DataFrame, N: int):\n",
    "\n",
    "  if N == 10:\n",
    "    top_n_features_list = top_10_features_list\n",
    "  elif N == 100:\n",
    "    top_n_features_list = top_100_features_list\n",
    "  elif N == 1000:\n",
    "    top_n_features_list = top_1000_features_list\n",
    "  else:\n",
    "    print('Please choose a right value for N (10, 100 or 1000)!')\n",
    "    return\n",
    "\n",
    "  assert N == len(top_n_features_list), \"The length of top_n_features_list should be equal with N!\"\n",
    "\n",
    "  features_matrix = np.zeros((df.shape[0], N))\n",
    "  for email_idx, row in df.iterrows():\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_n_features_list)):\n",
    "      word = top_n_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[email_idx, word_idx] = 1\n",
    "  return features_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj3VwYnI9kRj"
   },
   "source": [
    "Train a Bernoulli Naive Bayes classifier with binary features and get the precision score, recall score on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "674xNHNX9qv0",
    "outputId": "27b6427b-79f6-4193-ebd4-92b69455dba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB classiﬁer with Binary Features\n",
      "N = 10\n",
      "precision: 0.8888888888888888\n",
      "recall: 0.8163265306122449\n",
      "\n",
      "N = 100\n",
      "precision: 1.0\n",
      "recall: 0.673469387755102\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.6122448979591837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Bernoulli NB classiﬁer with Binary Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "x_train_10 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_10 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_10 = BernoulliNB()\n",
    "bernoulii_nb_binary_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_10.predict(x_test_10)\n",
    "bernoulii_nb_binary_10_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_10_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "x_train_100 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_100 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_100 = BernoulliNB()\n",
    "bernoulii_nb_binary_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_100.predict(x_test_100)\n",
    "bernoulii_nb_binary_100_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_100_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "x_train_1000 = extract_binary_features(lingspam_train_df, n)\n",
    "x_test_1000 = extract_binary_features(lingspam_test_df, n)\n",
    "bernoulii_nb_binary_1000 = BernoulliNB()\n",
    "bernoulii_nb_binary_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = bernoulii_nb_binary_1000.predict(x_test_1000)\n",
    "bernoulii_nb_binary_1000_precision = precision_score(y_test, y_pred)\n",
    "bernoulii_nb_binary_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(bernoulii_nb_binary_1000_precision))\n",
    "print(\"recall: {}\\n\".format(bernoulii_nb_binary_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRuf0-dc8og0"
   },
   "source": [
    "## Multinomial NB with Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjYeWKZO_9Xn"
   },
   "source": [
    "[Multinomial NB classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) is for multinomial models. It is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. We will use binary features in this multinomial NB classifier. The process of extrating binary features is the same with prior classifer.\n",
    "\n",
    "Next train a multinomial NB classifier with binary features and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "dDvcmA727hf5",
    "outputId": "8c3953b9-bdd7-4079-a601-0d9c06bc681f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB classiﬁer with Binary Features\n",
      "N = 10\n",
      "precision: 0.8888888888888888\n",
      "recall: 0.8163265306122449\n",
      "\n",
      "N = 100\n",
      "precision: 0.9782608695652174\n",
      "recall: 0.9183673469387755\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.9387755102040817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Multinomial NB classiﬁer with Binary Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "multinomial_nb_binary_10 = MultinomialNB()\n",
    "multinomial_nb_binary_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_10.predict(x_test_10)\n",
    "multinomial_nb_binary_10_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_10_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "multinomial_nb_binary_100 = MultinomialNB()\n",
    "multinomial_nb_binary_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_100.predict(x_test_100)\n",
    "multinomial_nb_binary_100_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_100_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "multinomial_nb_binary_1000 = MultinomialNB()\n",
    "multinomial_nb_binary_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_1000.predict(x_test_1000)\n",
    "multinomial_nb_binary_1000_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_binary_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_binary_1000_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_binary_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfWYw54q8s7j"
   },
   "source": [
    "## Multinomial NB with Term Frequency (TF) Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJXgVq4A_9-L"
   },
   "source": [
    "In this multinomial NB classifer, we will use term frequency (TF) features. In other words, we will use word occurence count. TF feature $x_i$ represents the times term $i$ appears in a document/email.\n",
    "\n",
    "Next, I will extract TF features from datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PwDUKjG98Bfa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "def extract_tf_features(df: DataFrame, N: int):\n",
    "\n",
    "  if N == 10:\n",
    "    top_n_features_list = top_10_features_list\n",
    "  elif N == 100:\n",
    "    top_n_features_list = top_100_features_list\n",
    "  elif N == 1000:\n",
    "    top_n_features_list = top_1000_features_list\n",
    "  else:\n",
    "    print('Please choose a right value for N (10, 100 or 1000)!')\n",
    "    return\n",
    "\n",
    "  assert N == len(top_n_features_list), \"The length of top_n_features_list should be equal with N!\"\n",
    "\n",
    "  features_matrix = np.zeros((df.shape[0], N))\n",
    "  for email_idx, row in df.iterrows():\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_n_features_list)):\n",
    "      word = top_n_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[email_idx, word_idx] = email_body.count(word)\n",
    "  return features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHrWrNsFSfGW"
   },
   "source": [
    "Train a Multinomial Naive Bayes classifier with term frequency features and get the precision score, recall score on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "J_aJio-BSifR",
    "outputId": "f407faac-65ad-4c5b-d72e-6dd9fc57f59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB classiﬁer with TF Features\n",
      "N = 10\n",
      "precision: 0.8518518518518519\n",
      "recall: 0.9387755102040817\n",
      "\n",
      "N = 100\n",
      "precision: 0.9787234042553191\n",
      "recall: 0.9387755102040817\n",
      "\n",
      "N = 1000\n",
      "precision: 1.0\n",
      "recall: 0.9387755102040817\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Multinomial NB classiﬁer with TF Features\")\n",
    "\n",
    "\n",
    "n = 10\n",
    "x_train_10 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_10 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_10 = MultinomialNB()\n",
    "multinomial_nb_tf_10.fit(x_train_10, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_10.predict(x_test_10)\n",
    "multinomial_nb_tf_10_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_10_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_10_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_10_recall))\n",
    "\n",
    "\n",
    "n = 100\n",
    "x_train_100 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_100 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_100 = MultinomialNB()\n",
    "multinomial_nb_tf_100.fit(x_train_100, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_100.predict(x_test_100)\n",
    "multinomial_nb_tf_100_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_100_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_100_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_100_recall))\n",
    "\n",
    "\n",
    "n = 1000\n",
    "x_train_1000 = extract_tf_features(lingspam_train_df, n)\n",
    "x_test_1000 = extract_tf_features(lingspam_test_df, n)\n",
    "multinomial_nb_tf_1000 = MultinomialNB()\n",
    "multinomial_nb_tf_1000.fit(x_train_1000, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_tf_1000.predict(x_test_1000)\n",
    "multinomial_nb_tf_1000_precision = precision_score(y_test, y_pred)\n",
    "multinomial_nb_tf_1000_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"N = {}\".format(n))\n",
    "print(\"precision: {}\".format(multinomial_nb_tf_1000_precision))\n",
    "print(\"recall: {}\\n\".format(multinomial_nb_tf_1000_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzhKXa9W8yqE"
   },
   "source": [
    "## SVM based Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG4sU4-H_-kP"
   },
   "source": [
    "In this section, I will design and implement a SVM based spam filter. The top-1000 features will be selected on terms of the highest Information Gain (IG) scores. I will use TF feature selection method as the function *extract_tf_features()* did. Also, cross validation will be employed to select the best model.\n",
    "\n",
    "Let's train SVM models with different hyperparameters and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z_EzvWtKZWkQ",
    "outputId": "361e3957-fd8b-408e-e43a-60e6ad59cb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM classiﬁer with TF Features\n",
      "Kernel = linear, C = 1\n",
      "precision: 0.7924528301886793\n",
      "recall: 0.8571428571428571\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.45458198, 0.41315246, 0.39673972, 0.33351684, 0.43068576]), 'score_time': array([0.02998853, 0.02638769, 0.01669574, 0.0317862 , 0.03013778]), 'test_precision': array([0.95238095, 0.98795181, 0.94117647, 0.97647059, 0.95294118]), 'test_recall': array([0.91954023, 0.94252874, 0.93023256, 0.96511628, 0.94186047])}\n",
      "best cross validation precision: 0.9879518072289156\n",
      "best cross validation recall: 0.9651162790697675\n",
      "\n",
      "Kernel = linear, C = 2\n",
      "precision: 0.7777777777777778\n",
      "recall: 0.8571428571428571\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.39812255, 0.37205935, 0.39114666, 0.30168295, 0.35313821]), 'score_time': array([0.03211689, 0.03205633, 0.02447844, 0.03632784, 0.01873207]), 'test_precision': array([0.95238095, 0.97619048, 0.94117647, 0.97647059, 0.96428571]), 'test_recall': array([0.91954023, 0.94252874, 0.93023256, 0.96511628, 0.94186047])}\n",
      "best cross validation precision: 0.9764705882352941\n",
      "best cross validation recall: 0.9651162790697675\n",
      "\n",
      "Kernel = poly, C = 1\n",
      "precision: 1.0\n",
      "recall: 0.14285714285714285\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([1.68338037, 1.59097123, 1.55535603, 1.56995559, 1.58112431]), 'score_time': array([0.26678181, 0.21027255, 0.27680469, 0.26835084, 0.29959965]), 'test_precision': array([0.97142857, 1.        , 1.        , 1.        , 1.        ]), 'test_recall': array([0.3908046 , 0.10344828, 0.24418605, 0.3255814 , 0.25581395])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.39080459770114945\n",
      "\n",
      "Kernel = poly, C = 2\n",
      "precision: 1.0\n",
      "recall: 0.20408163265306123\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([1.55997801, 1.46848416, 1.45128727, 2.19864249, 1.51007462]), 'score_time': array([0.32369399, 0.22137666, 0.65870571, 0.39176869, 0.29246187]), 'test_precision': array([0.97222222, 1.        , 1.        , 1.        , 1.        ]), 'test_recall': array([0.40229885, 0.17241379, 0.25581395, 0.36046512, 0.26744186])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.40229885057471265\n",
      "\n",
      "Kernel = rbf, C = 1\n",
      "precision: 1.0\n",
      "recall: 0.673469387755102\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([2.23029518, 1.43398261, 2.09301353, 2.41990185, 1.99572468]), 'score_time': array([0.38791347, 0.56663465, 0.81998968, 0.63016582, 0.37922716]), 'test_precision': array([0.98360656, 1.        , 0.98275862, 1.        , 1.        ]), 'test_recall': array([0.68965517, 0.71264368, 0.6627907 , 0.72093023, 0.74418605])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.7441860465116279\n",
      "\n",
      "Kernel = rbf, C = 2\n",
      "precision: 1.0\n",
      "recall: 0.7551020408163265\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([1.91252255, 1.56395388, 1.12837839, 1.30192065, 1.88709307]), 'score_time': array([0.4051528 , 0.39769769, 0.35215855, 0.44741797, 0.3663137 ]), 'test_precision': array([0.97014925, 1.        , 0.98387097, 1.        , 1.        ]), 'test_recall': array([0.74712644, 0.7816092 , 0.70930233, 0.76744186, 0.81395349])}\n",
      "best cross validation precision: 1.0\n",
      "best cross validation recall: 0.813953488372093\n",
      "\n",
      "Kernel = sigmoid, C = 1\n",
      "precision: 0.85\n",
      "recall: 0.6938775510204082\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.96731615, 0.84716153, 0.66164351, 0.67588496, 0.69138527]), 'score_time': array([0.05728793, 0.09227705, 0.03998995, 0.06154704, 0.05772495]), 'test_precision': array([0.69662921, 0.72972973, 0.78461538, 0.79746835, 0.71428571]), 'test_recall': array([0.71264368, 0.62068966, 0.59302326, 0.73255814, 0.63953488])}\n",
      "best cross validation precision: 0.7974683544303798\n",
      "best cross validation recall: 0.7325581395348837\n",
      "\n",
      "Kernel = sigmoid, C = 2\n",
      "precision: 0.8571428571428571\n",
      "recall: 0.7346938775510204\n",
      "\n",
      "Perform 5-fold Cross Validation on training set:\n",
      "{'fit_time': array([0.5675993 , 1.14826202, 1.77283025, 1.29525733, 0.71419978]), 'score_time': array([0.03086782, 0.37771773, 0.18602347, 0.07540703, 0.17782187]), 'test_precision': array([0.67021277, 0.73684211, 0.71232877, 0.73863636, 0.66666667]), 'test_recall': array([0.72413793, 0.64367816, 0.60465116, 0.75581395, 0.6744186 ])}\n",
      "best cross validation precision: 0.7386363636363636\n",
      "best cross validation recall: 0.7558139534883721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "print(\"SVM classiﬁer with TF Features\")\n",
    "\n",
    "n = 1000\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "c = [1, 2]\n",
    "\n",
    "x_train = extract_tf_features(lingspam_train_df, n)\n",
    "x_test = extract_tf_features(lingspam_test_df, n)\n",
    "\n",
    "# train svm models\n",
    "for k in kernel:\n",
    "  for reg in c:\n",
    "    svm_model = svm.SVC(kernel=k, C=reg).fit(x_train, y_train)\n",
    "\n",
    "    y_pred = svm_model.predict(x_test)\n",
    "    svm_precision = precision_score(y_test, y_pred)\n",
    "    svm_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Kernel = {}, C = {}\".format(k, reg))\n",
    "    print(\"precision: {}\".format(svm_precision))\n",
    "    print(\"recall: {}\\n\".format(svm_recall))\n",
    "\n",
    "    print(\"Perform 5-fold Cross Validation on training set:\")\n",
    "    scores = cross_validate(\n",
    "        svm_model, \n",
    "        x_train, \n",
    "        y_train, \n",
    "        cv=5,\n",
    "        scoring=('precision', 'recall')\n",
    "        )\n",
    "    print(scores)\n",
    "\n",
    "    cv_best_precision = max(scores['test_precision'])\n",
    "    cv_best_reall = max(scores['test_recall'])\n",
    "    print(\"best cross validation precision: {}\".format(cv_best_precision))\n",
    "    print(\"best cross validation recall: {}\\n\".format(cv_best_reall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNtHpUri9KHx"
   },
   "source": [
    "## Adversarial Classification based Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2HDX_WK__IA"
   },
   "source": [
    "Adversarial Classification is an approach to update NB based e-mail spam ﬁlters in response to attacks that try to evade a basic NB ﬁlter. I will implement the techniques presented in this [paper](https://dl.acm.org/doi/10.1145/1014052.1014066). More specifically, the following assumptions are made:\n",
    "* The baseline NB classiﬁer (Multinomial NB with binary features) uses the top-10 terms identiﬁed using the IG metric and using Boolean features.\n",
    "* The adversary uses the ADD-WORDS strategy. Cost of adding a word is 1. The attacker seeks to ﬁnd the minimum cost solution such that each spam email in the test set gets classiﬁed as legitimate by the baseline NB classiﬁer.\n",
    "* Adversary cannot modify negative instances, which means adversary can only modify spam emails in the testset.\n",
    "* Update the baseline NB classiﬁer in response to the attacker’s strategy above. The defender pays a unit price for both false positives and false negatives.\n",
    "\n",
    "\n",
    "Before attackers make modifications to test emails, let's calculate the False Negative rate of the baseline NB classifier through confusion matrix. Here, the baseline model is multinomial NB with binary features (N=10). Assume positve class is spam email class, negative class is legit email class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "lhrdWMUcUHk1",
    "outputId": "a15b5323-bff0-435b-9fe3-ddfdef1d95e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline nb classifier accuracy rate: 0.9553264604810997, precision: 0.8333333333333334, recall: 0.9183673469387755\n",
      "confusion matrix: \n",
      "[[233   9]\n",
      " [  4  45]]\n",
      "tn: 233, fp: 9, fn: 4, tp: 45\n",
      "fpr: 0.0371900826446281, fnr: 0.08163265306122448\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n = 10\n",
    "x_train = extract_tf_features(lingspam_train_df, n)\n",
    "x_test = extract_binary_features(lingspam_test_df, n)\n",
    "multinomial_nb_binary_baseline = MultinomialNB()\n",
    "multinomial_nb_binary_baseline.fit(x_train, y_train)\n",
    "\n",
    "y_pred = multinomial_nb_binary_baseline.predict(x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "before_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(fpr, before_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrNuJrugrNMb"
   },
   "source": [
    "First, let's get all spam emails in the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "MgBuAYZlK31O",
    "outputId": "3da8cd90-7592-424c-a983-a6de6962a5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam email size: 49\n",
      "spam email list: [62, 63, 64, 68, 83, 84, 85, 87, 91, 92, 93, 109, 111, 112, 113, 114, 127, 128, 129, 130, 131, 146, 148, 149, 153, 154, 169, 170, 173, 176, 177, 190, 191, 193, 194, 195, 196, 197, 206, 213, 214, 215, 216, 217, 219, 220, 237, 238, 240]\n"
     ]
    }
   ],
   "source": [
    "spam_email_list = []\n",
    "\n",
    "for email_idx, row in lingspam_test_df.iterrows():\n",
    "  if row['is_spam'] == 1:\n",
    "    spam_email_list.append(email_idx)\n",
    "\n",
    "spam_email_size = len(spam_email_list)\n",
    "print(\"spam email size: {}\".format(spam_email_size))\n",
    "print(\"spam email list: {}\".format(spam_email_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0KX8qcqGffU"
   },
   "source": [
    "Next perform \bADD-WORDS strategy for the adversary on these spam emails. The attacker seeks to ﬁnd the minimum cost solution such that each spam email in the test set gets classiﬁed as legitimate by the baseline NB classiﬁer. I will add words to spam emails in testset so that they can fool the classifier. The words are chosen from top-10 terms ranked by the IG metric. The \bADD-WORDS strategy is like a greedy algorithm here, once the prediction result of a spam email flips, stop adding words.\n",
    "\n",
    "The cost of adding a word is 1. I will calculate the average cost of the attacker’s modiﬁcations, averaged over all spam emails in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R2B4T-CmriWc",
    "outputId": "4aab7db9-f383-4c8e-df1f-e336e0b31d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing ADD-WORDS strategy on testset...\n",
      "email index: 62\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 63\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 64\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 68\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 1.]]\n",
      "cost: 3\n",
      "email index: 83\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 1.]]\n",
      "cost: 3\n",
      "email index: 84\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 85\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 87\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 91\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 92\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 93\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 109\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 111\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 112\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 5\n",
      "email index: 113\n",
      "original feature matrix: [[0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n",
      "cost: 0\n",
      "email index: 114\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 127\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 128\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 2\n",
      "email index: 129\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 130\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 131\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 146\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 148\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 4\n",
      "email index: 149\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 153\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 154\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 169\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 5\n",
      "email index: 170\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 173\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 176\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 177\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 190\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 1. 0. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 4\n",
      "email index: 191\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 0\n",
      "email index: 193\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 2\n",
      "email index: 194\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 195\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 196\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 197\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]]\n",
      "cost: 5\n",
      "email index: 206\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 0. 0. 0. 1.]]\n",
      "cost: 2\n",
      "email index: 213\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 214\n",
      "original feature matrix: [[0. 0. 0. 0. 0. 1. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 0. 1. 0.]]\n",
      "cost: 5\n",
      "email index: 215\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 216\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "cost: 1\n",
      "email index: 217\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 219\n",
      "original feature matrix: [[0. 1. 1. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "cost: 4\n",
      "email index: 220\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "email index: 237\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 1. 0.]]\n",
      "cost: 3\n",
      "email index: 238\n",
      "original feature matrix: [[0. 1. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "modified featurex matrix: [[1. 1. 1. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "cost: 3\n",
      "email index: 240\n",
      "original feature matrix: [[0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "modified featurex matrix: [[1. 0. 1. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "cost: 1\n",
      "all cost on testset: 99\n",
      "average cost: 2.020408163265306\n"
     ]
    }
   ],
   "source": [
    "print('performing ADD-WORDS strategy on testset...')\n",
    "all_cost = 0\n",
    "modified_x_test = x_test\n",
    "\n",
    "for email_idx, row in lingspam_test_df.iterrows():\n",
    "  if email_idx in spam_email_list:\n",
    "    cost = 0\n",
    "    print('email index: {}'.format(email_idx))\n",
    "    features_matrix = np.zeros((1, 10))\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_10_features_list)):\n",
    "      word = top_10_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[0, word_idx] = 1\n",
    "    \n",
    "    print('original feature matrix: {}'.format(features_matrix))\n",
    "\n",
    "    while multinomial_nb_binary_baseline.predict(features_matrix) != 0:\n",
    "      idx = next((i for i, x in enumerate(features_matrix[0]) if x == 0), None)\n",
    "      if idx == None:\n",
    "        break\n",
    "      features_matrix[0, idx] = 1\n",
    "      cost += 1\n",
    "    \n",
    "    all_cost += cost\n",
    "    modified_x_test[email_idx] = features_matrix\n",
    "    print('modified featurex matrix: {}'.format(features_matrix))\n",
    "    print('cost: {}'.format(cost))\n",
    "  \n",
    "avg_cost = all_cost / spam_email_size\n",
    "print('all cost on testset: {}'.format(all_cost))\n",
    "print('average cost: {}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bB0SG1WNxpA2"
   },
   "source": [
    "Evaluate baseline classifier on modified testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "obqnwbzi1Gcg",
    "outputId": "996fa8f9-8935-4f5b-b0cc-d89b4076055d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After the attacker's modifications to test emails\n",
      "baseline nb classifier accuracy rate: 0.8144329896907216, precision: 0.3076923076923077, recall: 0.08163265306122448\n",
      "confusion matrix: \n",
      "[[233   9]\n",
      " [ 45   4]]\n",
      "tn: 233, fp: 9, fn: 45, tp: 4\n",
      "fpr: 0.0371900826446281, fnr: 0.9183673469387755\n"
     ]
    }
   ],
   "source": [
    "after_y_pred = multinomial_nb_binary_baseline.predict(modified_x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, after_y_pred)\n",
    "precision = precision_score(y_test, after_y_pred)\n",
    "recall = recall_score(y_test, after_y_pred)\n",
    "conf_mat = confusion_matrix(y_test, after_y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "after_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"After the attacker's modifications to test emails\")\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(fpr, after_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H9aQjNRLZRp"
   },
   "source": [
    "Apply the same ADD-WORDS strategy on training set, then retrain the baseline classifier and evaluate the updated baseline classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "UqF-f_guMBlw",
    "outputId": "73b892bd-c5fe-4b52-bef5-3b00f06e4339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam email size: 432\n",
      "spam email list: [21, 38, 84, 85, 86, 92, 93, 94, 103, 104, 111, 112, 116, 117, 132, 133, 137, 138, 139, 153, 154, 155, 157, 158, 159, 160, 168, 169, 170, 171, 178, 179, 180, 187, 189, 190, 198, 199, 208, 209, 212, 213, 214, 228, 229, 231, 278, 281, 354, 359, 360, 361, 362, 370, 374, 375, 384, 385, 386, 387, 401, 402, 404, 405, 431, 432, 434, 436, 444, 445, 446, 447, 457, 458, 459, 460, 471, 472, 473, 474, 482, 483, 484, 485, 494, 495, 496, 497, 507, 508, 509, 510, 513, 514, 515, 516, 582, 583, 591, 593, 605, 606, 614, 616, 625, 626, 636, 637, 647, 649, 654, 656, 666, 668, 671, 677, 678, 684, 689, 693, 694, 700, 705, 709, 713, 719, 720, 729, 733, 735, 755, 764, 774, 785, 803, 805, 814, 815, 838, 849, 854, 856, 866, 867, 904, 918, 920, 923, 939, 942, 952, 956, 958, 968, 969, 971, 972, 986, 988, 990, 1002, 1003, 1016, 1017, 1018, 1036, 1039, 1040, 1053, 1054, 1059, 1061, 1064, 1073, 1075, 1076, 1080, 1081, 1092, 1094, 1098, 1099, 1101, 1117, 1118, 1119, 1125, 1126, 1135, 1136, 1144, 1151, 1157, 1174, 1189, 1198, 1202, 1205, 1218, 1242, 1250, 1254, 1268, 1272, 1281, 1285, 1286, 1295, 1296, 1306, 1307, 1308, 1309, 1319, 1320, 1326, 1327, 1328, 1330, 1344, 1345, 1363, 1365, 1366, 1367, 1387, 1388, 1389, 1390, 1391, 1403, 1404, 1407, 1408, 1426, 1427, 1428, 1430, 1444, 1445, 1448, 1453, 1454, 1456, 1459, 1460, 1462, 1474, 1476, 1477, 1479, 1481, 1482, 1499, 1500, 1503, 1505, 1506, 1508, 1519, 1520, 1523, 1524, 1581, 1591, 1605, 1621, 1629, 1666, 1669, 1670, 1671, 1672, 1675, 1692, 1693, 1694, 1696, 1709, 1711, 1713, 1714, 1722, 1726, 1727, 1728, 1729, 1730, 1736, 1737, 1740, 1741, 1756, 1758, 1759, 1761, 1765, 1766, 1769, 1783, 1785, 1786, 1791, 1792, 1793, 1804, 1805, 1806, 1807, 1808, 1827, 1828, 1829, 1908, 1919, 1922, 1937, 1938, 1939, 1953, 1954, 1956, 1960, 1961, 1979, 1980, 1984, 1986, 1996, 1997, 2001, 2002, 2003, 2018, 2019, 2021, 2060, 2063, 2068, 2079, 2090, 2105, 2108, 2122, 2125, 2129, 2132, 2143, 2156, 2157, 2162, 2163, 2179, 2180, 2181, 2183, 2185, 2186, 2195, 2196, 2200, 2201, 2216, 2217, 2221, 2222, 2235, 2238, 2242, 2244, 2258, 2261, 2264, 2267, 2268, 2271, 2283, 2284, 2291, 2292, 2300, 2301, 2306, 2307, 2317, 2318, 2319, 2320, 2321, 2322, 2333, 2335, 2345, 2346, 2347, 2348, 2365, 2373, 2374, 2376, 2379, 2392, 2395, 2396, 2397, 2399, 2400, 2413, 2505, 2507, 2527, 2529, 2545, 2546, 2547, 2548, 2549, 2550, 2559, 2560, 2561, 2562, 2575, 2576, 2577, 2579, 2591, 2593, 2595, 2596, 2597, 2598]\n",
      "performing ADD-WORDS strategy on trainset...\n",
      "all cost on trainset: 1043\n",
      "average cost: 2.4143518518518516\n",
      "Updateing baseline classifier...\n",
      "baseline nb classifier accuracy rate: 0.865979381443299, precision: 0.5806451612903226, recall: 0.7346938775510204\n",
      "confusion matrix: \n",
      "[[216  26]\n",
      " [ 13  36]]\n",
      "tn: 216, fp: 26, fn: 13, tp: 36\n",
      "fpr: 0.10743801652892562, fnr: 0.2653061224489796\n"
     ]
    }
   ],
   "source": [
    "train_spam_email_list = []\n",
    "\n",
    "for email_idx, row in lingspam_train_df.iterrows():\n",
    "  if row['is_spam'] == 1:\n",
    "    train_spam_email_list.append(email_idx)\n",
    "\n",
    "train_spam_email_size = len(train_spam_email_list)\n",
    "print(\"spam email size: {}\".format(train_spam_email_size))\n",
    "print(\"spam email list: {}\".format(train_spam_email_list))\n",
    "\n",
    "\n",
    "print('performing ADD-WORDS strategy on trainset...')\n",
    "all_cost_train = 0\n",
    "modified_x_train = x_train\n",
    "\n",
    "for email_idx, row in lingspam_train_df.iterrows():\n",
    "  if email_idx in train_spam_email_list:\n",
    "    cost = 0\n",
    "    # print('email index: {}'.format(email_idx))\n",
    "    features_matrix = np.zeros((1, 10))\n",
    "    email_body = row['email_body'].split(' ')\n",
    "    for word_idx in range(len(top_10_features_list)):\n",
    "      word = top_10_features_list[word_idx]\n",
    "      if word in email_body:\n",
    "        features_matrix[0, word_idx] = 1\n",
    "    \n",
    "    # print('original feature matrix: {}'.format(features_matrix))\n",
    "\n",
    "    while multinomial_nb_binary_baseline.predict(features_matrix) != 0:\n",
    "      idx = next((i for i, x in enumerate(features_matrix[0]) if x == 0), None)\n",
    "      if idx == None:\n",
    "        break\n",
    "      features_matrix[0, idx] = 1\n",
    "      cost += 1\n",
    "    \n",
    "    all_cost_train += cost\n",
    "    modified_x_train[email_idx] = features_matrix\n",
    "    # print('modified featurex matrix: {}'.format(features_matrix))\n",
    "    # print('cost: {}'.format(cost))\n",
    "  \n",
    "avg_cost_train = all_cost_train / train_spam_email_size\n",
    "print('all cost on trainset: {}'.format(all_cost_train))\n",
    "print('average cost: {}'.format(avg_cost_train))\n",
    "\n",
    "print('Updateing baseline classifier...')\n",
    "multinomial_nb_binary_updated = MultinomialNB()\n",
    "multinomial_nb_binary_updated.fit(modified_x_train, y_train)\n",
    "\n",
    "updated_y_pred = multinomial_nb_binary_updated.predict(modified_x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, updated_y_pred)\n",
    "precision = precision_score(y_test, updated_y_pred)\n",
    "recall = recall_score(y_test, updated_y_pred)\n",
    "conf_mat = confusion_matrix(y_test, updated_y_pred, labels = [0, 1])\n",
    "\n",
    "tn = conf_mat[0][0]\n",
    "fn = conf_mat[1][0]\n",
    "tp = conf_mat[1][1]\n",
    "fp = conf_mat[0][1]\n",
    "# False positive rate\n",
    "updated_fpr = fp / (fp + tn)\n",
    "# False negative rate\n",
    "updated_fnr = fn / (tp + fn)\n",
    "\n",
    "print(\"baseline nb classifier accuracy rate: {}, precision: {}, recall: {}\".format(acc_score, precision, recall))\n",
    "print(\"confusion matrix: \\n{}\".format(conf_mat))\n",
    "print(\"tn: {}, fp: {}, fn: {}, tp: {}\".format(tn, fp, fn, tp))\n",
    "print(\"fpr: {}, fnr: {}\".format(updated_fpr, updated_fnr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2dhRnLe9Xe2"
   },
   "source": [
    "# Result and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTYqaCDG__2A"
   },
   "source": [
    "The evaluation results of Naive Bayes classifers are in the table below. It has 9 rows, one for each classifier and N combination ($N = {10, 100, 1000}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMGx2rIR9fKH"
   },
   "source": [
    "Naive Bayes Classifier | N | Precision | Recall\n",
    "--- | --- | --- | ---\n",
    "Bernoulli NB classiﬁer with binary features | 10 | 0.8888888888888888 | 0.8163265306122449\n",
    "Bernoulli NB classiﬁer with binary features | 100 | 1.0 | 0.673469387755102\n",
    "Bernoulli NB classiﬁer with binary features | 1000 | 1.0 | 0.6122448979591837\n",
    "Multinomial NB with binary features | 10 | 0.8888888888888888 | 0.8163265306122449\n",
    "Multinomial NB with binary features | 100 | 0.9782608695652174 | 0.9183673469387755\n",
    "Multinomial NB with binary features | 1000 | 1.0 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 10 | 0.8518518518518519 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 100 | 0.9787234042553191 | 0.9387755102040817\n",
    "Multinomial NB with term frequency (TF) features | 1000 | 1.0 | 0.9387755102040817\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK1r-n8qL8J1"
   },
   "source": [
    "The evaluation results of SVM classifers with different hyperparameters are in the table below. 4 different kernels are tested, they are ['linear', 'poly', 'rbf', 'sigmoid']. $C$ represents regularization parameter. Here, $C \\in [1, 2]$. The strength of the regularization is inversely proportional to $C$. SVM classifers use top-1000 features selected based on information gain and use term frequency features. Also, 5-fold cross validation is performed on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjCmSF0tMejy"
   },
   "source": [
    "SVM Classifier | Kernel | C | Cross Validation | Precision | Recall\n",
    "--- | --- | --- | --- | --- | ---\n",
    "SVM classifier with TF features | linear | 1 | False | 0.7924528301886793 | 0.8571428571428571\n",
    "SVM classifier with TF features | linear | 1 | True | 0.9879518072289156 | 0.9651162790697675\n",
    "SVM classifier with TF features | linear | 2 | False | 0.7777777777777778 | 0.8571428571428571\n",
    "SVM classifier with TF features | linear | 2 | True | 0.9764705882352941 | 0.9651162790697675\n",
    "SVM classifier with TF features | poly | 1 | False | 1.0 | 0.14285714285714285\n",
    "SVM classifier with TF features | poly | 1 | True | 1.0 | 0.39080459770114945\n",
    "SVM classifier with TF features | poly | 2 | False | 1.0 | 0.20408163265306123\n",
    "SVM classifier with TF features | poly | 2 | True | 1.0 | 0.40229885057471265\n",
    "SVM classifier with TF features | rbf | 1 | False | 1.0 | 0.673469387755102\n",
    "SVM classifier with TF features | rbf | 1 | True | 1.0 | 0.7441860465116279\n",
    "SVM classifier with TF features | rbf | 2 | False | 1.0 | 0.7551020408163265\n",
    "SVM classifier with TF features | rbf | 2 | True | 1.0 | 0.813953488372093\n",
    "SVM classifier with TF features | sigmoid | 1 | False | 0.85 | 0.6938775510204082\n",
    "SVM classifier with TF features | sigmoid | 1 | True | 0.7974683544303798 | 0.7325581395348837\n",
    "SVM classifier with TF features | sigmoid | 2 | False | 0.8571428571428571 | 0.7346938775510204\n",
    "SVM classifier with TF features | sigmoid | 2 | True | 0.7386363636363636 | 0.7558139534883721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LOVOoesT1Rd"
   },
   "source": [
    "The evaluation results of Adversarial Attack classifers are as following. Here, baseline NB classifier is multinomial NB with binary features (N=10, top-10 features are selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "30ui54Bf47rB",
    "outputId": "52453ddc-983b-420f-b3a8-7f35d4654e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Rate of the baseline NB classifier before attacker's modification: 0.08163265306122448\n",
      "False Negative Rate of the baseline NB classifier after attacker's modification: 0.9183673469387755\n",
      "Average cost of attacker's modifications: 2.020408163265306\n",
      "False Negative Rate of updated NB classifier: 0.2653061224489796\n",
      "False Positvive Rate of updated NB classifier: 0.10743801652892562\n"
     ]
    }
   ],
   "source": [
    "print('False Negative Rate of the baseline NB classifier before attacker\\'s modification: {}'.format(before_fnr))\n",
    "print('False Negative Rate of the baseline NB classifier after attacker\\'s modification: {}'.format(after_fnr))\n",
    "print('Average cost of attacker\\'s modifications: {}'.format(avg_cost))\n",
    "print('False Negative Rate of updated NB classifier: {}'.format(updated_fnr))\n",
    "print('False Positvive Rate of updated NB classifier: {}'.format(updated_fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "email_spam_filter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
